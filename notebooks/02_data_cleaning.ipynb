{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8908b813",
   "metadata": {},
   "source": [
    "# 02. Data Cleaning & Enrichment\n",
    "\n",
    "**Objective:** Transform raw staging data into high-quality analytics-ready data.\n",
    "**Key Actions:**\n",
    "1.  **Regex Cleaning:** Removed artifacts like `***` and `@@@` from text fields.\n",
    "2.  **Geospatial Enrichment:** Mapped `Ciudad` to a new `Pais` column for Power BI mapping.\n",
    "3.  **Type Casting:** Converted strings to `Float` and `DateTime`.\n",
    "4.  **Logical Normalization:** Fixed inconsistent `Tipo_Producto` classifications using a master dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85edf11",
   "metadata": {},
   "source": [
    "### Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "496123cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import re # For Regex\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "from database.db_connection import get_db_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74251bf1",
   "metadata": {},
   "source": [
    "### Load Staging Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05789c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created engine for database: riwi_ventas_db\n",
      "Loaded 1000000 rows.\n"
     ]
    }
   ],
   "source": [
    "engine = get_db_engine()\n",
    "df = pd.read_sql(\"SELECT * FROM raw_sales\", engine)\n",
    "print(f\"Loaded {len(df)} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5165f13e",
   "metadata": {},
   "source": [
    "### Standard Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "538241dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates before: 15191\n",
      "Rows remaining: 940258\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 940258 entries, 0 to 999999\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count   Dtype         \n",
      "---  ------           --------------   -----         \n",
      " 0   Ciudad           940258 non-null  object        \n",
      " 1   Fecha            940258 non-null  datetime64[ns]\n",
      " 2   Producto         940258 non-null  object        \n",
      " 3   Tipo_Producto    940258 non-null  object        \n",
      " 4   Cantidad         940258 non-null  float64       \n",
      " 5   Precio_Unitario  940258 non-null  float64       \n",
      " 6   Tipo_Venta       940258 non-null  object        \n",
      " 7   Tipo_Cliente     940258 non-null  object        \n",
      " 8   Descuento        940258 non-null  float64       \n",
      " 9   Costo_Envio      940258 non-null  float64       \n",
      " 10  Total            940258 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(5), object(5)\n",
      "memory usage: 86.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Remove Special Characters (Regex) from Text Columns\n",
    "text_cols = ['Ciudad', 'Producto', 'Tipo_Producto', 'Tipo_Venta', 'Tipo_Cliente']\n",
    "for col in text_cols:\n",
    "    # Remove anything that isn't a letter, number, or space\n",
    "    df[col] = df[col].astype(str).str.replace(r'[^\\w\\s]', '', regex=True).str.strip().str.title()\n",
    "\n",
    "# Fix Numeric Columns (Coerce errors to NaN)\n",
    "numeric_cols = ['Cantidad', 'Precio_Unitario', 'Descuento', 'Costo_Envio', 'Total']\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Fix Date\n",
    "df['Fecha'] = pd.to_datetime(df['Fecha'], errors='coerce')\n",
    "\n",
    "# Handle Nulls & Duplicates\n",
    "print(f\"Duplicates before: {df.duplicated().sum()}\")\n",
    "df_clean = df.drop_duplicates().dropna().copy()\n",
    "print(f\"Rows remaining: {len(df_clean)}\")\n",
    "\n",
    "# Check types\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb530af",
   "metadata": {},
   "source": [
    "### Data Enrichment (Adding Country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "744cd5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cities without country: ['None']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ciudad</th>\n",
       "      <th>Pais</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rosario</td>\n",
       "      <td>Argentina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Miami</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bogotá</td>\n",
       "      <td>Colombia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Madrid</td>\n",
       "      <td>Spain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monterrey</td>\n",
       "      <td>Mexico</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Ciudad       Pais\n",
       "0    Rosario  Argentina\n",
       "1      Miami        USA\n",
       "2     Bogotá   Colombia\n",
       "3     Madrid      Spain\n",
       "4  Monterrey     Mexico"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add Country Mapping to Cleaned DataFrame\n",
    "\n",
    "# Define the Mapping Dictionary \n",
    "city_to_country = {\n",
    "    'Antofagasta': 'Chile',\n",
    "    'Santiago': 'Chile',\n",
    "    'Valparaíso': 'Chile',\n",
    "    'Valparaiso': 'Chile', # Just in case without accent\n",
    "    'Concepción': 'Chile',\n",
    "    'Concepcion': 'Chile',\n",
    "    \n",
    "    'Bogotá': 'Colombia',\n",
    "    'Bogota': 'Colombia',\n",
    "    'Medellín': 'Colombia',\n",
    "    'Medellin': 'Colombia',\n",
    "    'Cali': 'Colombia',\n",
    "    'Barranquilla': 'Colombia',\n",
    "    'Cartagena': 'Colombia',\n",
    "    'Bucaramanga': 'Colombia',\n",
    "    'Pereira': 'Colombia',\n",
    "    \n",
    "    'Ciudad De México': 'Mexico',\n",
    "    'Ciudad De Mexico': 'Mexico',\n",
    "    'Monterrey': 'Mexico',\n",
    "    'Guadalajara': 'Mexico',\n",
    "    'Puebla': 'Mexico',\n",
    "    'Tijuana': 'Mexico',\n",
    "    \n",
    "    'Buenos Aires': 'Argentina',\n",
    "    'Córdoba': 'Argentina',\n",
    "    'Cordoba': 'Argentina',\n",
    "    'Rosario': 'Argentina',\n",
    "    'Mendoza': 'Argentina',\n",
    "    \n",
    "    'Lima': 'Peru',\n",
    "    'Cusco': 'Peru',\n",
    "    'Arequipa': 'Peru',\n",
    "    'Trujillo': 'Peru',\n",
    "    \n",
    "    'Madrid': 'Spain',\n",
    "    'Barcelona': 'Spain',\n",
    "    'Valencia': 'Spain',\n",
    "    'Sevilla': 'Spain',\n",
    "    \n",
    "    'New York': 'USA',\n",
    "    'Los Angeles': 'USA',\n",
    "    'Chicago': 'USA',\n",
    "    'Houston': 'USA',\n",
    "    'Miami': 'USA'\n",
    "}\n",
    "\n",
    "# Apply Mapping to df_clean specifically\n",
    "df_clean['Ciudad'] = df_clean['Ciudad'].str.title().str.strip()\n",
    "df_clean['Pais'] = df_clean['Ciudad'].map(city_to_country)\n",
    "\n",
    "# Validation: Check for nulls in 'Pais'\n",
    "missing_countries = df_clean[df_clean['Pais'].isnull()]['Ciudad'].unique()\n",
    "print(f\"Cities without country: {missing_countries}\")\n",
    "\n",
    "# 4. Preview to confirm column exists\n",
    "display(df_clean[['Ciudad', 'Pais']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd1d0f2",
   "metadata": {},
   "source": [
    "### Final Polish (Remove None & Fix Categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ac2d668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before global cleaning: 940258\n",
      "Found 4292 'None' values in column 'Ciudad'. Removing...\n",
      "Found 4314 'None' values in column 'Producto'. Removing...\n",
      "Found 4303 'None' values in column 'Tipo_Producto'. Removing...\n",
      "Found 4200 'None' values in column 'Tipo_Venta'. Removing...\n",
      "Found 4082 'None' values in column 'Tipo_Cliente'. Removing...\n",
      "Rows after global cleaning: 919067\n",
      "Product categories standardized.\n"
     ]
    }
   ],
   "source": [
    "# Identify text columns (where 'None' string might exist)\n",
    "text_cols = ['Ciudad', 'Pais', 'Producto', 'Tipo_Producto', 'Tipo_Venta', 'Tipo_Cliente']\n",
    "\n",
    "# Filter out rows where ANY of these columns equals \"None\"\n",
    "# This is stricter than dropna() to deal with the string \"None\"\n",
    "print(f\"Rows before global cleaning: {len(df_clean)}\")\n",
    "\n",
    "for col in text_cols:\n",
    "    count_none = len(df_clean[df_clean[col] == 'None'])\n",
    "    if count_none > 0:\n",
    "        print(f\"Found {count_none} 'None' values in column '{col}'. Removing...\")\n",
    "        df_clean = df_clean[df_clean[col] != 'None']\n",
    "\n",
    "print(f\"Rows after global cleaning: {len(df_clean)}\")\n",
    "\n",
    "# Standardize Product Categories (Logical Fix)\n",
    "product_mapping = {\n",
    "    'Arepa': 'Harinas',\n",
    "    'Leche': 'Lácteos',\n",
    "    'Queso': 'Lácteos',\n",
    "    'Chocolate': 'Dulces',\n",
    "    'Yogurt': 'Lácteos',\n",
    "    'Galletas': 'Dulces',\n",
    "    'Pan': 'Panadería',\n",
    "    'Gaseosa': 'Bebidas',\n",
    "    'Té': 'Bebidas',\n",
    "    'Café': 'Bebidas',\n",
    "    'Mantequilla': 'Lácteos'\n",
    "}\n",
    "\n",
    "# Overwrite Product_Type with the correct static mapping\n",
    "df_clean['Tipo_Producto'] = df_clean['Producto'].map(product_mapping).fillna(df_clean['Tipo_Producto'])\n",
    "\n",
    "print(\"Product categories standardized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095c8bfe",
   "metadata": {},
   "source": [
    "### DATA INTEGRITY GATE: Mathematical Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19b2ad4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integrity Check: Dropping 19043 rows due to mathematical inconsistency.\n",
      "Final clean row count: 900024\n"
     ]
    }
   ],
   "source": [
    "# Calculate Expected Total based on formula\n",
    "# Formula: (Unit_Price * Quantity * (1 - Discount)) + Shipping\n",
    "expected_total = (df_clean['Precio_Unitario'] * df_clean['Cantidad'] * (1 - df_clean['Descuento'])) + df_clean['Costo_Envio']\n",
    "\n",
    "# Calculate Deviation\n",
    "# Compare the CSV Total vs Expected Total\n",
    "deviation = abs(df_clean['Total'] - expected_total)\n",
    "\n",
    "# Filter Data\n",
    "# Allow a small tolerance of $1.0 for floating point rounding differences\n",
    "tolerance = 1.0\n",
    "valid_rows = deviation < tolerance\n",
    "\n",
    "# Count dropped rows\n",
    "dropped_math_count = (~valid_rows).sum()\n",
    "print(f\"Integrity Check: Dropping {dropped_math_count} rows due to mathematical inconsistency.\")\n",
    "\n",
    "# Apply filter\n",
    "df_clean = df_clean[valid_rows].copy()\n",
    "\n",
    "print(f\"Final clean row count: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41a20ef",
   "metadata": {},
   "source": [
    "### Load to Data Warehouse (Clean Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd3a3f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 900024 rows to 'clean_sales'...\n",
      "Success! Pipeline finished.\n"
     ]
    }
   ],
   "source": [
    "table_name = 'clean_sales'\n",
    "\n",
    "try:\n",
    "    print(f\"Saving {len(df_clean)} rows to '{table_name}'...\")\n",
    "    df_clean.to_sql(\n",
    "        name=table_name,\n",
    "        con=engine,\n",
    "        if_exists='replace',\n",
    "        index=False,\n",
    "        chunksize=10000\n",
    "    )\n",
    "    print(\"Success! Pipeline finished.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving to DB: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827e4049",
   "metadata": {},
   "source": [
    "## Export\n",
    "Clean data exported to PostgreSQL table `clean_sales`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
